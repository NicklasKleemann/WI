# -*- coding: utf-8 -*-
"""WebIntelligence_SelfStudyII.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VA-0pnMQvZQoFBg291sTIdHrOMTebQXb

Make sure to make a copy of this Notebook to work with
"""

# pip install wikipedia-api

import wikipediaapi

def fetch_wikipedia_category_members(category_name, max_items=100, user_agent="MyFetcher/1.0"):
    wiki_wiki = wikipediaapi.Wikipedia(
        language='en',
        user_agent=user_agent
    )

    category_page = wiki_wiki.page(category_name)

    if not category_page.exists():
        print(f"Category {category_name} does not exist.")
        return []

    items = []
    def extract_members(category_members, max_items):
        for c in category_members.values():
            if len(items) >= max_items:
                break  # Stop when we have enough items
            if (c.ns == wikipediaapi.Namespace.MAIN and
                not c.title.startswith("List of") and
                c.title.lower() not in ['scientist', 'actor', 'sportsperson']):

                items.append(c.title)
                print(f"Added: {c.title}")
            # Dive into subcategories recursively
            elif c.ns == wikipediaapi.Namespace.CATEGORY:
                extract_members(c.categorymembers, max_items)

    extract_members(category_page.categorymembers, max_items)
    return items

def fetch_all_categories():
    user_agent = "MyCategoryFetcher/1.0 (https://example.com/; contact@example.com)"
    scientists = fetch_wikipedia_category_members("Category:Scientists", max_items=100, user_agent=user_agent)
    actors = fetch_wikipedia_category_members("Category:Actors", max_items=100, user_agent=user_agent)
    sportspersons = fetch_wikipedia_category_members("Category:Sportspeople", max_items=100, user_agent=user_agent)
    result = {
        "scientists": scientists,
        "actors": actors,
        "sportspersons": sportspersons
    }

    return result

import os

def load_or_fetch_categories():
    """Load from cached files if they exist, otherwise fetch from Wikipedia."""
    categories = ["scientists", "actors", "sportspersons"]
    files_exist = all(os.path.exists(f"{cat}_list.txt") for cat in categories)
    
    if files_exist:
        print("Loading from cached files...")
        result = {}
        for cat in categories:
            with open(f"{cat}_list.txt", "r") as f:
                result[cat] = [line.strip() for line in f if line.strip()]
            print(f"  {cat}: {len(result[cat])} entries from {cat}_list.txt")
        return result
    else:
        print("Fetching from Wikipedia (will cache for next run)...")
        category_data = fetch_all_categories()
        for category, names in category_data.items():
            with open(f"{category}_list.txt", "w") as file:
                for name in names:
                    file.write(name + "\n")
            print(f"  {category}: {len(names)} saved to {category}_list.txt")
        return category_data

category_data = load_or_fetch_categories()

def fetch_wikipedia_abstract(page_title, user_agent="MyAbstractFetcher/1.0"):
    wiki_wiki = wikipediaapi.Wikipedia(
        language='en',
        user_agent=user_agent
    )
    print(f"Fetching abstract for {page_title}")
    page = wiki_wiki.page(page_title)
    if not page.exists():
        print(f"Page '{page_title}' does not exist.")
        return None
    return page.summary

"""Tasks:

1. Create a small dataset by randomly selecting 10 scientists, 10 actors and 10 sportspersons from the list above.
2. Extract the abstract of all these 30 pages using the function given above
3. Compute the similarity between all these pages (all possible combinations) using normalization (lower-case and punctuation removal), tokenization, TF-IDF, and cosine similarity.
4. Rank all the pairs of similar webpages (Hint: Most similar pair will be ranked 1)
5. Analyze the output of your results
"""

# =============================================================================
# SOLUTIONS - Using wi_toolkit
# =============================================================================
import random
import wi_toolkit as wt

# Ex 1-2: Select 10 each and get abstracts
print("\nEx 1-2: Select 10 each and get abstracts")
selected = (random.sample(category_data["scientists"], 10) + 
            random.sample(category_data["actors"], 10) + 
            random.sample(category_data["sportspersons"], 10))
print(f"Selected {len(selected)} pages, fetching abstracts from wiki.")
abstracts = {n: fetch_wikipedia_abstract(n) for n in selected}
abstracts = {k: v for k, v in abstracts.items() if v}
print(f"Fetched abstracts for {len(abstracts)} pages")

# Ex 3: TF-IDF + Cosine similarity
print("\nEx 3: TF-IDF + Cosine similarity")
print("Normalizing abstracts: lowercase and remove punctuation/numbers.")
docs = [wt.prepare_text(a, verbose=False) for a in abstracts.values()]
tfidf, vocab, idf = wt.compute_tfidf(docs, verbose=False)
print(f"computed TF-IDF for {len(docs)} documents")
similarities = wt.pairwise_scores(tfidf, list(abstracts.keys()), verbose=False)
print(f"computed pairwise similarities for {len(similarities)} pairs.")

# Ex 4-5: Rank and analyze
print("\nEx 4-5: Rank and analyze")
print(f"ranking {len(similarities)} pairs.")
ranked = wt.rank_pairs(similarities, verbose=False)
print("\nTop 10 most similar pairs:")
for i, (n1, n2, sim) in enumerate(ranked[:10], 1):
    print(f"  {i}. {n1} <-> {n2}: {sim:.4f}")