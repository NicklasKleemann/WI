# -*- coding: utf-8 -*-
"""WebIntelligence_SelfStudy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14f6oFn-zErkwVR4Def0AClOEV2xZ7P6G
"""

# pip install wikipedia-api

import wikipediaapi

def fetch_wikipedia_category_members(category_name, max_items=50, user_agent="MyFetcher/1.0"):
    wiki_wiki = wikipediaapi.Wikipedia(
        language='en',
        user_agent=user_agent
    )

    category_page = wiki_wiki.page(category_name)

    if not category_page.exists():
        print(f"Category {category_name} does not exist.")
        return []

    items = []
    def extract_members(category_members, max_items):
        for c in category_members.values():
            if len(items) >= max_items:
                break  # Stop when we have enough items
            if (c.ns == wikipediaapi.Namespace.MAIN and
                not c.title.startswith("List of") and
                c.title.lower() not in ['scientist', 'actor', 'sportsperson', 'musician']):

                items.append(c.title)
                print(f"Added: {c.title}")
            # Dive into subcategories recursively
            elif c.ns == wikipediaapi.Namespace.CATEGORY:
                extract_members(c.categorymembers, max_items)

    extract_members(category_page.categorymembers, max_items)
    return items

def fetch_all_categories():
    user_agent = "MyCategoryFetcher/1.0 (https://example.com/; contact@example.com)"
    scientists = fetch_wikipedia_category_members("Category:Scientists", max_items=50, user_agent=user_agent)
    actors = fetch_wikipedia_category_members("Category:Actors", max_items=50, user_agent=user_agent)
    sportspersons = fetch_wikipedia_category_members("Category:Sportspeople", max_items=50, user_agent=user_agent)
    musicians = fetch_wikipedia_category_members("Category:Musicians", max_items=50, user_agent=user_agent)
    result = {
        "scientists": scientists,
        "actors": actors,
        "sportspersons": sportspersons,
        "musicians": musicians
    }

    return result


category_data = fetch_all_categories()

print(category_data)
for category, names in category_data.items():
    with open(f"{category}_list.txt", "w") as file:
        for name in names:
            file.write(name + "\n")

    print(f"{category.capitalize()} saved to {category}_list.txt.")

def fetch_wikipedia_abstract(page_title, user_agent="MyAbstractFetcher/1.0"):
    wiki_wiki = wikipediaapi.Wikipedia(
        language='en',
        user_agent=user_agent
    )

    page = wiki_wiki.page(page_title)
    if not page.exists():
        print(f"Page '{page_title}' does not exist.")
        return None
    return page.summary

# pip install torch

"""# **Exercise**

Tasks


1.   Get the abstracts for 50 scientists, 50 actors, 50 sportsperson, and 50 musicians
2.   Consider the 4 categories as 4 classes. Split the dataset into 80% train, 10% validation (dev), and 10% test set. Fine-tune a pretrained BERT model for classification task and report the accuracy, precision, recall and F1 score.

"""

# =============================================================================
# SOLUTIONS - Using wi_toolkit
# =============================================================================
import wi_toolkit as wt

# Ex 1: Get abstracts (uses category_data from above)
all_texts, all_labels = [], []
for i, cat in enumerate(["scientists", "actors", "sportspersons", "musicians"]):
    for name in category_data[cat]:
        abstract = fetch_wikipedia_abstract(name)
        if abstract:
            all_texts.append(abstract)
            all_labels.append(i)

# Ex 2: Split dataset
train, val, test = wt.split_data(all_texts, all_labels, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1)

# For BERT fine-tuning, use: from transformers import BertForSequenceClassification, Trainer