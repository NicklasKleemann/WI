# -*- coding: utf-8 -*-
"""WebIntelligence2024_SelfStudyI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IXvbVb0zM1VB6G0XXjs8L7hVczBVEmUz
"""

import wikipediaapi

# Get Wikipedia page text using the API (cleaner than scraping)
wiki = wikipediaapi.Wikipedia(user_agent='WebIntelligenceStudy/1.0', language='en')
page = wiki.page("Barack Obama")
full_text = page.text

print(f"Fetched {len(full_text)} characters from Wikipedia")
print(full_text[:500])  # Preview first 500 chars

"""### Use the NLTK library to perform the following tasks on the extracted text and explore other features as well. The tasks include:

1. Tokenization
2. Normalization
3. Stopword removal
4. Manually review the output of these steps to verify if they meet your expectations. If not, create your own function to clean the data.
Note: It's not necessary to preserve Named Entities. For example, after completing the preprocessing steps, the string "Albert Einstein" being reduced to ["albert", "einstein"] is considered to be correct.
5. Implement Zipf's law on the cleaned data without using any pre-built Python libraries.
6. Visualize the results (hint: use matplotlib or another plotting library).
7. Eliminate all numbers from the text (including dates and years).
8. Apply the implemented Zipf's law to the data after number removal and plot the results.
"""

# =============================================================================
# SOLUTIONS - Using wi_toolkit
# =============================================================================
import wi_toolkit as wt
import matplotlib.pyplot as plt
from collections import Counter

# Ex 1-4: Full preprocessing pipeline
#includes normize -> tokenize -> filter -> stem
tokens = wt.clean_pipeline(full_text, remove_numbers=False, verbose=False)

# Ex 5-6: Zipf's law
freq = Counter(tokens)
plt.loglog(range(1, len(freq)+1), sorted(freq.values(), reverse=True))
plt.xlabel('Rank'); plt.ylabel('Frequency'); plt.title("Zipf's Law")
plt.savefig('zipf_law.png'); plt.show()

# Ex 7-8: Without numbers
tokens_no_nums = wt.clean_pipeline(full_text[:100], remove_numbers=True, verbose=True)

#Final results
print(f"\nTop 10: {freq.most_common(10)}")
print(f"\nTop 10 no numbers: {Counter(tokens_no_nums).most_common(10)}")
