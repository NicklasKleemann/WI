# -*- coding: utf-8 -*-
"""WebIntelligence_SelfStudyIII and IV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19mo9HbJYHJyxq4fHIwgz8n39qqP6M89a
"""

# pip install wikipedia-api

import wikipediaapi

def fetch_wikipedia_category_members(category_name, max_items=100, user_agent="MyFetcher/1.0"):
    wiki_wiki = wikipediaapi.Wikipedia(
        language='en',
        user_agent=user_agent
    )

    category_page = wiki_wiki.page(category_name)

    if not category_page.exists():
        print(f"Category {category_name} does not exist.")
        return []

    items = []
    def extract_members(category_members, max_items):
        for c in category_members.values():
            if len(items) >= max_items:
                break  # Stop when we have enough items
            if (c.ns == wikipediaapi.Namespace.MAIN and
                not c.title.startswith("List of") and
                c.title.lower() not in ['scientist', 'actor', 'sportsperson']):

                items.append(c.title)
                print(f"Added: {c.title}")
            # Dive into subcategories recursively
            elif c.ns == wikipediaapi.Namespace.CATEGORY:
                extract_members(c.categorymembers, max_items)

    extract_members(category_page.categorymembers, max_items)
    return items

def fetch_all_categories():
    user_agent = "MyCategoryFetcher/1.0 (https://example.com/; contact@example.com)"
    scientists = fetch_wikipedia_category_members("Category:Scientists", max_items=100, user_agent=user_agent)
    actors = fetch_wikipedia_category_members("Category:Actors", max_items=100, user_agent=user_agent)
    sportspersons = fetch_wikipedia_category_members("Category:Sportspeople", max_items=100, user_agent=user_agent)
    result = {
        "scientists": scientists,
        "actors": actors,
        "sportspersons": sportspersons
    }

    return result

import os

def load_or_fetch_categories():
    """Load from cached files if they exist, otherwise fetch from Wikipedia."""
    categories = ["scientists"]
    files_exist = all(os.path.exists(f"{cat}_list.txt") for cat in categories)
    
    if files_exist:
        print("Loading from cached files...")
        result = {}
        for cat in categories:
            with open(f"{cat}_list.txt", "r") as f:
                result[cat] = [line.strip() for line in f if line.strip()]
            print(f"  {cat}: {len(result[cat])} entries from {cat}_list.txt")
        return result
    else:
        print("Fetching from Wikipedia (will cache for next run)...")
        category_data = fetch_all_categories()
        for category, names in category_data.items():
            with open(f"{category}_list.txt", "w") as file:
                for name in names:
                    file.write(name + "\n")
            print(f"  {category}: {len(names)} saved to {category}_list.txt")
        return category_data

category_data = load_or_fetch_categories()

def fetch_wikipedia_abstract(page_title, user_agent="WebIntelligenceStudy/1.0"):
    wiki_wiki = wikipediaapi.Wikipedia(
        language='en',
        user_agent=user_agent
    )
    print(f"  Fetching: {page_title}")
    page = wiki_wiki.page(page_title)
    if not page.exists():
        return None
    return page.summary

"""# **Exercise III**


Continuation from the last self-study exercise:

1. Build an inverted index on the abstract of the 10 scientists you extracted last week. Consider each Wikipedia pages as a document, i.e., the corpus consists of 10 documents.
2. Develop a merge algorithm to search the words "scientist and award" in the documents.
"""

#code of Exercise IV

# =============================================================================
# SOLUTIONS - Using wi_toolkit
# =============================================================================
import wi_toolkit as wt

# Ex III.1: Build inverted index on 10 scientists
import random
print("\nEx III.1: Build inverted index on 10 random scientists")
scientists_10 = random.sample(category_data["scientists"], 10)

abstracts = {n: fetch_wikipedia_abstract(n) for n in scientists_10}
abstracts = {k: v for k, v in abstracts.items() if v}
print(f"Fetched abstracts for {len(abstracts)} pages")  
docs = [wt.prepare_text(a, verbose=False) for a in abstracts.values()]
index = wt.build_index(docs, list(abstracts.keys()), verbose=false)
print(f"Built inverted index with {len(index)} unique terms")

# Ex III.2: Develop a merge algorithm to search "scientist and award"
print("\nEx III.2: Merge algorithm for 'scientist AND award'")
result = wt.query_index(index, "scientist award", op='AND')
print(f"Result: {result if result else 'No documents found'}")

