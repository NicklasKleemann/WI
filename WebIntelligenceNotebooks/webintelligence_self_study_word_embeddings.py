# -*- coding: utf-8 -*-
"""WebIntelligence_Self Study_Word Embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D-O3LU7SWTQQ1hMHZsxD2qyDIoWkpuHe
"""

# !pip install wikipedia-api

import os
import json
import wikipediaapi
import wi_toolkit as wt

# Wikipedia API Setup
user_agent = "WebIntelligenceSelfStudy/1.0 (contact@example.com)"
wiki = wikipediaapi.Wikipedia(language='en', user_agent=user_agent)

def get_page_abstract(page_title):
    """Fetch summary from Wikipedia."""
    page = wiki.page(page_title)
    if page.exists():
        return page.summary
    return None

def load_or_fetch_abstracts(titles, cache_file="entities_abstracts.json"):
    """Load abstracts from cache or fetch if missing."""
    if os.path.exists(cache_file):
        print(f"Loading abstracts from {cache_file}...")
        with open(cache_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    print("Fetching abstracts from Wikipedia...")
    abstracts = {}
    for title in titles:
        print(f"  Fetching: {title}")
        summary = get_page_abstract(title)
        if summary:
            abstracts[title] = summary
    
    # Save to cache
    with open(cache_file, 'w', encoding='utf-8') as f:
        json.dump(abstracts, f, indent=4)
    print(f"Saved {len(abstracts)} abstracts to {cache_file}")
    return abstracts

def main():
    # 1. The 30 entities from the assignment
    page_titles = [
        "Marie Curie", "Albert Einstein", "Ada Lovelace", "Richard Feynman",
        "Dorothy Hodgkin", "Alan Turing", "Chien-Shiung Wu", "Niels Bohr",
        "Grace Hopper", "Linus Pauling", "Meryl Streep", "Leonardo DiCaprio",
        "Emma Thompson", "Tom Hanks", "Kate Winslet", "Shah Rukh Khan",
        "Benedict Cumberbatch", "Nicole Kidman", "Penélope Cruz", "Brad Pitt",
        "Megan Rapinoe", "Sachin Tendulkar", "Brian Lara", "Shane Warne",
        "Serena Williams", "Lucy Bronze", "Mithali Raj", "Lionel Messi",
        "Roger Federer", "Cristiano Ronaldo"
    ]
    
    abstracts = load_or_fetch_abstracts(page_titles)
    print(f"Loaded {len(abstracts)} entities.\n")

def main():
    # 1. The 30 entities from the assignment
    page_titles = [
        "Marie Curie", "Albert Einstein", "Ada Lovelace", "Richard Feynman",
        "Dorothy Hodgkin", "Alan Turing", "Chien-Shiung Wu", "Niels Bohr",
        "Grace Hopper", "Linus Pauling", "Meryl Streep", "Leonardo DiCaprio",
        "Emma Thompson", "Tom Hanks", "Kate Winslet", "Shah Rukh Khan",
        "Benedict Cumberbatch", "Nicole Kidman", "Penélope Cruz", "Brad Pitt",
        "Megan Rapinoe", "Sachin Tendulkar", "Brian Lara", "Shane Warne",
        "Serena Williams", "Lucy Bronze", "Mithali Raj", "Lionel Messi",
        "Roger Federer", "Cristiano Ronaldo"
    ]
    
    # Ex 1: Get abstracts
    abstracts = load_or_fetch_abstracts(page_titles)
    print(f"Loaded {len(abstracts)} entities.\n")
    
    # Ex 2: Generate embeddings
    # Use toolkit function to load model
    model = wt.load_word_vectors('GoogleNews-vectors-negative300.bin', fallback_model='glove-wiki-gigaword-50')
    
    # Print dimensions of word vectors
    sample_word = "scientist"
    if sample_word in model:
        print(f"Word vector dimension: {len(model[sample_word])}")
    
    entity_embeddings = {}
    valid_names = []
    embedding_matrix = []
    
    print("\nGenerating entity embeddings...")
    for name, abstract in abstracts.items():
        # Use toolkit function to compute document embedding
        doc_emb = wt.compute_doc_embedding(abstract, model, verbose=False)
        if doc_emb is not None:
            entity_embeddings[name] = doc_emb
            valid_names.append(name)
            embedding_matrix.append(doc_emb)
            
    if embedding_matrix:
        print(f"Generated {len(embedding_matrix)} document embeddings.")
        print(f"Document embedding dimension: {len(embedding_matrix[0])}")

    # Ex 3: Compute similarities and find top-5
    print("\nEx 3: Top-5 Most Similar Entities")
    for i, name in enumerate(valid_names):
        target_vec = embedding_matrix[i]
        print(f"\nEntity: {name}")
        # Find top 6 (one will be self)
        results = wt.find_nearest(target_vec, embedding_matrix, valid_names, k=6, verbose=False)
        
        # Filter out self and show top 5
        count = 0
        for other_name, score in results:
            if other_name == name:
                continue
            if count < 5:
                print(f"  {count+1}. {other_name}: {score:.4f}")
                count += 1

if __name__ == "__main__":
    main()
